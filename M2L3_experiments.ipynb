{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded9488a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete. Langsmith and Groq clients initialized.\n",
      "Simple 'correct_label' evaluator defined.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# CELL 1: Setup & Simple Custom Evaluator\n",
    "# -----------------------------------------------------\n",
    "import os\n",
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import Client\n",
    "from langchain_groq import ChatGroq \n",
    "from pydantic import BaseModel, Field\n",
    "from langsmith.schemas import Example, Run # Keeping these for cleaner reference in other cells\n",
    "\n",
    "# Suppress the specific LangChain Deprecation Warning (good practice)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"langchain\")\n",
    "\n",
    "# Load environment variables (LANGCHAIN_API_KEY, GROQ_API_KEY, etc.)\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initialize Groq Client and Langsmith Client\n",
    "# Set low temp for evaluation consistency\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
    "llm_client = ChatGroq(model=MODEL_NAME, temperature=0.0) \n",
    "client = Client()\n",
    "\n",
    "print(\"Setup Complete. Langsmith and Groq clients initialized.\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Simple Custom Evaluator\n",
    "# -----------------------------------------------------\n",
    "def correct_label(inputs: dict, reference_outputs: dict, outputs: dict) -> dict:\n",
    "  \"\"\"A very simple evaluator comparing model output to a reference 'output'.\"\"\"\n",
    "  # NOTE: We are comparing the 'output' keys from the provided dicts\n",
    "  score = outputs.get(\"output\") == reference_outputs.get(\"output\") \n",
    "  return {\"score\": int(score), \"key\": \"correct_label\"}\n",
    "\n",
    "print(\"Simple 'correct_label' evaluator defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "580c7216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Application defined using Groq Model: llama-3.3-70b-versatile\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# CELL 2: RAG Application Definition (Groq-Compatible)\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# --- GLOBAL CONFIGURATION (Will be updated later) ---\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\" \n",
    "MODEL_PROVIDER = \"groq\"\n",
    "APP_VERSION = 2.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "# Initial Groq client. This will be updated for the second experiment.\n",
    "llm_client = ChatGroq(model=MODEL_NAME)\n",
    "\n",
    "# --- MOCK Retriever (Simulating the Document Fetch) ---\n",
    "# This simulates the internal RAG component that returns retrieved documents\n",
    "def get_mock_retriever():\n",
    "    return [\n",
    "        Document(page_content=\"LangSmith is a platform for building and evaluating LLM applications.\"),\n",
    "        Document(page_content=\"Experiments in LangSmith allow comparison of different model versions against a single dataset.\"),\n",
    "        Document(page_content=\"The `evaluate` function runs your application and records all traces and metrics.\")\n",
    "    ]\n",
    "\n",
    "# --- RAG Pipeline Functions ---\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return get_mock_retriever()\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"}\n",
    "    ]\n",
    "    return call_groq(messages)\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\n",
    "        \"ls_provider\": MODEL_PROVIDER,\n",
    "        \"ls_model_name\": MODEL_NAME\n",
    "    }\n",
    ")\n",
    "def call_groq(messages: List[dict]):\n",
    "    # Uses the current global llm_client instance (either Llama or Mixtral)\n",
    "    response = llm_client.invoke(messages)\n",
    "    return response\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    # The evaluation function expects a string output\n",
    "    return response.content\n",
    "\n",
    "print(f\"RAG Application defined using Groq Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d00b2c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically selected dataset: M2L1 RAG Examples - 15f2633a\n",
      "Custom Evaluators defined.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# CELL 3: Experiment Setup (The Core Tweak Cell)\n",
    "# -----------------------------------------------------\n",
    "\n",
    "datasets_list = list(client.list_datasets(limit=1)) \n",
    "\n",
    "# Now use the corrected variable name in the conditional logic\n",
    "if not datasets_list:\n",
    "    # Use a safe fallback name\n",
    "    dataset_name = \"M2L1 RAG Examples - DEFAULT\" \n",
    "    print(\"WARNING: No dynamic dataset found. Using default placeholder name. (Check M2L1 completion)\")\n",
    "else:\n",
    "    # Safely access the first (most recent) dataset in the list\n",
    "    dataset_name = datasets_list[0].name\n",
    "print(f\"Automatically selected dataset: {dataset_name}\")\n",
    "\n",
    "# TWEAK 2 (Impressive Custom Evaluator): Checks adherence to the system prompt\n",
    "def is_three_sentences(reference_outputs: dict, outputs: dict) -> dict:\n",
    "    \"\"\"Evaluator that checks if the model output adheres to the max three-sentence constraint.\"\"\"\n",
    "    answer = outputs[\"output\"].strip()\n",
    "    # Simple sentence count based on common delimiters (. ! ?)\n",
    "    sentence_count = len([s for s in answer.split('.') if s.strip()]) \n",
    "    \n",
    "    # We check if the count is <= 3, enforcing the 'Use three sentences maximum' rule\n",
    "    score = (sentence_count <= 3) \n",
    "    return {\"key\": \"max_three_sentences_check\", \"score\": int(score)}\n",
    "\n",
    "# Original conciseness evaluator\n",
    "def is_concise_enough(reference_outputs: dict, outputs: dict) -> dict:\n",
    "    score = len(outputs[\"output\"]) < 1.5 * len(reference_outputs[\"output\"])\n",
    "    return {\"key\": \"is_concise\", \"score\": int(score)}\n",
    "\n",
    "def target_function(inputs: dict):\n",
    "    \"\"\"Wraps the RAG pipeline to match the `evaluate` function signature.\"\"\"\n",
    "    return langsmith_rag(inputs[\"question\"])\n",
    "\n",
    "print(\"Custom Evaluators defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e969b0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Baseline Experiment: llama-3.3-70b-versatile ---\n",
      "View the evaluation results for experiment: 'Groq-llama-3.3-70b-versatile-V2.0-Baseline-250607f6' at:\n",
      "https://smith.langchain.com/o/6072fe80-253a-475b-81f3-74f20971421c/datasets/30db0c67-cb69-4056-a271-1e22e70fab73/compare?selectedSessions=3ffd6f68-706a-4411-9de1-47d5e24dadea\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:04,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Experiment finished. Check Langsmith.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# CELL 4: Experiment 1: Groq Llama Baseline\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Run the first experiment with Groq Llama model (Baseline)\n",
    "print(f\"\\n--- Running Baseline Experiment: {MODEL_NAME} ---\")\n",
    "\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough, is_three_sentences], # Using our custom evaluator\n",
    "    experiment_prefix=f\"Groq-{MODEL_NAME}-V{APP_VERSION}-Baseline\",\n",
    "    num_repetitions=1,\n",
    "    metadata={\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"version\": APP_VERSION,\n",
    "        \"prompt_constraint\": \"max-three-sentences\"\n",
    "    }\n",
    ")\n",
    "print(\"Baseline Experiment finished. Check Langsmith.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dac2a958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Comparison Experiment: llama-3.3-70b-versatile (Reduced Load) ---\n",
      "View the evaluation results for experiment: 'Groq-llama-3.3-70b-versatile-V2.0-Comparison-LowLoad-732ee744' at:\n",
      "https://smith.langchain.com/o/6072fe80-253a-475b-81f3-74f20971421c/datasets/30db0c67-cb69-4056-a271-1e22e70fab73/compare?selectedSessions=a0cebd49-b59f-4751-a86b-fbeaa26c0a98\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:02,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison Experiment finished. Check Langsmith.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# CELL 5: Experiment 2: Groq Comparison (Reduced Load)\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# FIX: Keep the safe Llama model and drastically reduce repetitions/concurrency\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
    "llm_client = ChatGroq(model=MODEL_NAME) # Reset Groq client instance\n",
    "\n",
    "# Run the comparison experiment with minimal load (Repetitions reduced to 1, Concurrency set low)\n",
    "print(f\"\\n--- Running Comparison Experiment: {MODEL_NAME} (Reduced Load) ---\")\n",
    "\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough, is_three_sentences],\n",
    "    experiment_prefix=f\"Groq-{MODEL_NAME}-V{APP_VERSION}-Comparison-LowLoad\",\n",
    "    num_repetitions=1, # Safest setting\n",
    "    max_concurrency=2, # Keep concurrency very low\n",
    "    metadata={\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"version\": APP_VERSION,\n",
    "        \"prompt_constraint\": \"max-three-sentences\",\n",
    "        \"load_setting\": \"minimal\"\n",
    "    }\n",
    ")\n",
    "print(\"Comparison Experiment finished. Check Langsmith.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c2135eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Safe Concurrency Test ---\n",
      "View the evaluation results for experiment: 'Groq-SafeLoad-Reps1-Conc2-02aafc24' at:\n",
      "https://smith.langchain.com/o/6072fe80-253a-475b-81f3-74f20971421c/datasets/30db0c67-cb69-4056-a271-1e22e70fab73/compare?selectedSessions=7bdcc3c1-196f-40fd-858c-863aafde950c\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:05,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe Concurrency Test finished. Check Langsmith.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------\n",
    "# CELL 6: Testing Other Parameters (Safe Concurrency Test)\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# FIX: Simplify the load test to run fewer total calls to prevent rate limiting.\n",
    "print(\"\\n--- Running Safe Concurrency Test ---\")\n",
    "\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\"\n",
    "llm_client = ChatGroq(model=MODEL_NAME) # Reset model\n",
    "\n",
    "evaluate(\n",
    "    target_function,\n",
    "    data=dataset_name,\n",
    "    evaluators=[is_concise_enough],\n",
    "    experiment_prefix=f\"Groq-SafeLoad-Reps1-Conc2\",\n",
    "    num_repetitions=1, # Safest setting\n",
    "    max_concurrency=2, # Run 2 threads concurrently, 1 time each (2 total runs)\n",
    "    metadata={\n",
    "        \"test_type\": \"safe_load_test\",\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"repetitions\": 1,\n",
    "        \"concurrency\": 2\n",
    "    }\n",
    ")\n",
    "print(\"Safe Concurrency Test finished. Check Langsmith.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langsmith_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
