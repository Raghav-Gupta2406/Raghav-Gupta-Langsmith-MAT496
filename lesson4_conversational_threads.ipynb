{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9e73933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Question 2 ---\n",
      "How can I filter and view this specific conversation in the Langsmith UI?\n",
      "\n",
      "--- Answer 2 ---\n",
      "I don't know how to filter and view a specific conversation in the Langsmith UI. The provided context only discusses the @traceable decorator and adding metadata to traces. It does not mention filtering or viewing conversations in the Langsmith UI.\n",
      "\n",
      "\n",
      "Two traces were created with the same 'conversation_id'. Check Langsmith!\n"
     ]
    }
   ],
   "source": [
    "# 4. Second Turn: Continue the Thread\n",
    "\n",
    "question_2 = \"How can I filter and view this specific conversation in the Langsmith UI?\"\n",
    "\n",
    "# PASS THE QUESTIONS BY NAME and the SAME unique_thread_id in langsmith_extra metadata\n",
    "ai_answer_2 = langsmith_conversational_rag(\n",
    "    question=question_2, \n",
    "    langsmith_extra={\"metadata\": {\"conversation_id\": unique_thread_id}}\n",
    ")\n",
    "\n",
    "print(f\"\\n--- Question 2 ---\\n{question_2}\")\n",
    "print(f\"\\n--- Answer 2 ---\\n{ai_answer_2}\")\n",
    "\n",
    "print(\"\\n\\nTwo traces were created with the same 'conversation_id'. Check Langsmith!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daf7600e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Question 1 ---\n",
      "How do I trace my conversation history using Langsmith?\n",
      "\n",
      "--- Answer 1 ---\n",
      "You can use the @traceable decorator to log traces of your functions, including conversations, in LangSmith. To track conversation history, add the decorator to the relevant functions and optionally pass a 'metadata' dictionary for additional context. This will automatically track inputs, outputs, and errors, providing a record of your conversation history.\n"
     ]
    }
   ],
   "source": [
    "# 3. First Turn: Start the Conversation Thread (TWEAK)\n",
    "\n",
    "question_1 = \"How do I trace my conversation history using Langsmith?\"\n",
    "\n",
    "# TWEAK: Pass the unique_thread_id in langsmith_extra metadata\n",
    "ai_answer_1 = langsmith_conversational_rag(\n",
    "    question=question_1, \n",
    "    langsmith_extra={\"metadata\": {\"conversation_id\": unique_thread_id}}\n",
    ")\n",
    "\n",
    "print(f\"--- Question 1 ---\\n{question_1}\")\n",
    "print(f\"\\n--- Answer 1 ---\\n{ai_answer_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e64eef4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. RAG Pipeline Functions\n",
    "\n",
    "@traceable(run_type=\"retriever\", name=\"Retrieve Documents\")\n",
    "def retrieve_documents(question: str):\n",
    "    # This invokes the retriever defined in utils.py\n",
    "    return retriever.invoke(question)   \n",
    "\n",
    "@traceable(run_type=\"llm\", name=\"Final LLM Call (Groq)\")\n",
    "def call_llm(messages: List[dict]):\n",
    "    # This LLM call is automatically traced by LangChain/Groq\n",
    "    return llm_client.invoke(messages)\n",
    "\n",
    "@traceable(run_type=\"chain\", name=\"RAG Response Generation\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"}\n",
    "    ]\n",
    "    response = call_llm(messages)\n",
    "    return response\n",
    "\n",
    "@traceable(run_type=\"chain\", name=\"Root Conversational RAG\")\n",
    "def langsmith_conversational_rag(question: str, **kwargs):\n",
    "    # kwargs will contain the langsmith_extra metadata\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82b4b44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Unique Thread ID: 54483e69-9497-4f5f-a7ed-bc00d34bd039\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup Environment, Imports, and UUID\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import traceable \n",
    "from langchain_groq import ChatGroq \n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "import uuid\n",
    "import warnings # New import to handle warnings\n",
    "from utils import get_vector_db_retriever, RAG_SYSTEM_PROMPT\n",
    "\n",
    "# Suppress the specific LangChain Deprecation Warning\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"langchain\")\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv()\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\" # Using the confirmed, working Groq model\n",
    "llm_client = ChatGroq(model=MODEL_NAME)\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "# TWEAK: Generate a unique thread ID for the entire conversation\n",
    "unique_thread_id = str(uuid.uuid4())\n",
    "print(f\"Generated Unique Thread ID: {unique_thread_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langsmith_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
