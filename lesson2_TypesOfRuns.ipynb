{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a5cd209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Retriever Run Output ---\n",
      "[{'page_content': 'The solution required deleting the local .git folder.', 'type': 'Document', 'metadata': {'source': 'local_git_cache_v3'}}, {'page_content': 'A Personal Access Token (PAT) bypasses credential caching.', 'type': 'Document', 'metadata': {'source': 'local_git_cache_v3'}}, {'page_content': 'Re-initialize Git and force push to establish tracking.', 'type': 'Document', 'metadata': {'source': 'local_git_cache_v3'}}]\n"
     ]
    }
   ],
   "source": [
    "# 4. Retriever Runs: Fixing run_type and Document format\n",
    "\n",
    "def _convert_docs(results):\n",
    "  return [\n",
    "      {\n",
    "          \"page_content\": r,\n",
    "          \"type\": \"Document\", # FIX: Corrected missing key\n",
    "          \"metadata\": {\"source\": \"local_git_cache_v3\"} # TWEAK: Custom metadata\n",
    "      }\n",
    "      for r in results\n",
    "  ]\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"retriever\" # FIX: Set the correct run_type\n",
    ")\n",
    "def retrieve_docs(query):\n",
    "  # TWEAK: Customized document content to be about Git fix strategy\n",
    "  contents = [\"The solution required deleting the local .git folder.\", \"A Personal Access Token (PAT) bypasses credential caching.\", \"Re-initialize Git and force push to establish tracking.\"] \n",
    "  return _convert_docs(contents)\n",
    "\n",
    "print(\"\\n--- Retriever Run Output ---\")\n",
    "print(retrieve_docs(\"How to fix Git Push errors?\")) # TWEAK: Customized user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b26939a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Streaming Model Run Output ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'choices': [{'message': {'content': 'Hello, Ranger the Retriever is a great name.',\n",
       "     'role': 'assistant'}}]},\n",
       " {'choices': [{'message': {'content': ' I hope you have a nice day!',\n",
       "     'role': 'assistant'}}]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Streaming LLM Runs: Fixing the reduce_fn\n",
    "\n",
    "def _reduce_chunks(chunks: list):\n",
    "    # This reduction logic is for LangSmith, ensuring the streamed response is combined\n",
    "    all_text = \"\".join([chunk[\"choices\"][0][\"message\"][\"content\"] for chunk in chunks])\n",
    "    # The output format is now clean and avoids syntax issues:\n",
    "    return {\"choices\": [{\"message\": {\"content\": all_text, \"role\": \"assistant\"}}]}\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"llm\",\n",
    "    metadata={\"ls_provider\": \"my_provider\", \"ls_model_name\": \"my_model\"},\n",
    "    reduce_fn=_reduce_chunks # FIX: Added the required reduce_fn\n",
    ")\n",
    "def my_streaming_chat_model(messages: list):\n",
    "    # TWEAK: Customized chunk output based on the input\n",
    "    user_input = messages[1][\"content\"]\n",
    "    for chunk in [f\"Hello, {user_input} is a great name.\", \" I hope you have a nice day!\"]:\n",
    "        yield {\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"message\": {\n",
    "                        \"content\": chunk,\n",
    "                        \"role\": \"assistant\",\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "print(\"\\n--- Streaming Model Run Output ---\")\n",
    "list(\n",
    "    my_streaming_chat_model(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Please greet the user.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Ranger the Retriever\"}, # TWEAK: Changed animal input\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d30fce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Chat Model Run Output ---\n",
      "I'll simulate a call to a chat model. To start, I'll need to know what kind of input or prompt you'd like to give the model. Would you like to:\n",
      "\n",
      "1. Ask a question\n",
      "2. Provide a statement to discuss\n",
      "3. Engage in a role-playing scenario\n",
      "4. Something else (please specify)\n",
      "\n",
      "Please respond with the number of your chosen action or describe your desired interaction.\n"
     ]
    }
   ],
   "source": [
    "# 2. LLM Runs: Setting run_type=\"llm\" and adding custom example\n",
    "\n",
    "inputs = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a friendly barista assistant. Please confirm the customer's coffee order.\"},\n",
    "  {\"role\": \"user\", \"content\": \"I'd like a large cold brew with vanilla syrup.\"}, # TWEAK: Modified input for a coffee order\n",
    "]\n",
    "\n",
    "output = {\n",
    "  \"choices\": [\n",
    "      {\n",
    "          \"message\": {\n",
    "              \"role\": \"assistant\",\n",
    "              \"content\": \"One large cold brew with vanilla added to your order. Will that be all?\", # TWEAK: Modified output\n",
    "          }\n",
    "      }\n",
    "  ]\n",
    "}\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"llm\",  # FIX: Set the correct run_type\n",
    "    metadata={\n",
    "        \"ls_provider\": \"synthetic-groq-test\", \n",
    "        \"ls_model_name\": \"barista-bot-v1\" # TWEAK: Custom model name\n",
    "    } \n",
    ")\n",
    "def chat_model(messages: list):\n",
    "  # In a real app, this would be an actual Groq API call, but for the lesson,\n",
    "  # we return the expected output format for tracing.\n",
    "  return output\n",
    "\n",
    "print(\"--- Chat Model Run Output ---\")\n",
    "# To fully trace this LLM run, we pass it to LangChain's invocation method.\n",
    "from langchain.schema.messages import HumanMessage\n",
    "response = llm_client.invoke([HumanMessage(content=\"Simulate call to chat_model\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eacff06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Environment and Imports (Groq & Generic)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import traceable \n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "import json\n",
    "# Use Groq LLM client\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# --- Configuration ---\n",
    "load_dotenv()\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\" # Using a supported Groq model\n",
    "llm_client = ChatGroq(model=MODEL_NAME)\n",
    "nest_asyncio.apply()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langsmith_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
