{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "435ebcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Environment Variables and Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import traceable # Required for tracing!\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "# We use ChatGroq instead of OpenAI\n",
    "from langchain_groq import ChatGroq \n",
    "from utils import get_vector_db_retriever, RAG_SYSTEM_PROMPT\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "MODEL_PROVIDER = \"groq\"\n",
    "MODEL_NAME = \"llama-3.3-70b-versatile\" # Using a fast, standard Groq model\n",
    "\n",
    "# Initialize Groq client (LangChain handles authentication via ENV variable)\n",
    "llm_client = ChatGroq(model=MODEL_NAME)\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fde84250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Apply @traceable decorators with custom metadata (YOUR TWEAK)\n",
    "\n",
    "@traceable(\n",
    "    name=\"Retrieve Documents\",\n",
    "    metadata={\"vectordb_type\": \"FAISS\", \"k_neighbors\": 2} # TWEAK: Adding descriptive retrieval metadata\n",
    ")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)   \n",
    "\n",
    "\n",
    "@traceable(name=\"Generate Response Chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    # Call the Groq LLM client\n",
    "    response = llm_client.invoke(messages)\n",
    "    return response\n",
    "\n",
    "\n",
    "@traceable(\n",
    "    name=\"Final LLM Call\",\n",
    "    metadata={\"model_name\": MODEL_NAME, \"model_provider\": MODEL_PROVIDER} # TWEAK: Adding descriptive LLM metadata\n",
    ")\n",
    "def call_llm(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    "):\n",
    "    # The actual LLM invocation happens here and is traced automatically by LangChain,\n",
    "    # but we keep this function traced to show the metadata context.\n",
    "    return llm_client.invoke(messages)\n",
    "\n",
    "\n",
    "@traceable(name=\"Root RAG Pipeline\")\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    # Use call_llm to show the context in the trace\n",
    "    response = call_llm(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": RAG_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {question}. Context: {documents}\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f259e5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Question ---\n",
      "\n",
      "How can I trace with the @traceable decorator?\n",
      "\n",
      "--- Answer ---\n",
      "\n",
      "To trace with the @traceable decorator, you can add it to any function to automatically track its inputs, outputs, and errors in LangSmith. You can also pass a 'metadata' dictionary to the @traceable decorator to add metadata to traces. Additionally, metadata can be added at runtime using the 'langsmith_extra' parameter.\n",
      "\n",
      "--- Question (Metadata Check) ---\n",
      "\n",
      "How do I add Metadata to a Run with @traceable?\n",
      "\n",
      "--- Answer ---\n",
      "\n",
      "To add metadata to a run with @traceable, you can pass a 'metadata' dictionary to the @traceable decorator. This allows you to include additional information with your traces. You can also use the 'langsmith_extra' parameter to add metadata at runtime.\n"
     ]
    }
   ],
   "source": [
    "# 3. Execute the pipeline and view the trace in Langsmith\n",
    "question = \"How can I trace with the @traceable decorator?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "\n",
    "print(\"--- Question ---\\n\")\n",
    "print(question)\n",
    "print(\"\\n--- Answer ---\\n\")\n",
    "print(ai_answer)\n",
    "\n",
    "question_metadata = \"How do I add Metadata to a Run with @traceable?\"\n",
    "ai_answer_metadata = langsmith_rag(question_metadata)\n",
    "\n",
    "print(\"\\n--- Question (Metadata Check) ---\\n\")\n",
    "print(question_metadata)\n",
    "print(\"\\n--- Answer ---\\n\")\n",
    "print(ai_answer_metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langsmith_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
